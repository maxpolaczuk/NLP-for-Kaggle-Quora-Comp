{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAGGLE COMPETITION\n",
    "### QUORA QUESTION DUPLICATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# change dir:\n",
    "os.chdir('/home/max/Downloads/glove.twitter.27B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in txt file of pretrained word vectors using GloVe on\n",
    "# 27Billion tweets\n",
    "# More casual vocabulary size of 1.2M words\n",
    "\n",
    "fname = 'glove.twitter.27B.100d.txt' # 100dimensional takes about 30s\n",
    "\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# remove whitespace characters:\n",
    "content = [x.strip() for x in content] \n",
    "\n",
    "# make a list of words only:\n",
    "words = [x.split()[0] for x in content]\n",
    "\n",
    "# create a lookup table from the list of words:\n",
    "words = {k: str(v).lower() for v, k in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make function to locate each word idx in a sentence from glove vector:\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def find_glove(sentence):\n",
    "    ''' take in the sentece of words, then try and locate their index'''\n",
    "    # convert sentect to list of words:\n",
    "    sentence = text_to_word_sequence(sentence)\n",
    "    \n",
    "    # set up empty array for GloVe indexs\n",
    "    idxs = []\n",
    "    for w in sentence:\n",
    "        # go over each word and add its index:\n",
    "        idxs.append(words[w])\n",
    "        \n",
    "    \n",
    "    return idxs,sentence\n",
    "        \n",
    "# make function to get glove vectors at\n",
    "def glove_seq(sequen):\n",
    "    seq = [] # empty list for us to create the sequence\n",
    "    for i in sequen:\n",
    "        # i is the ID number of the word\n",
    "        # iterate over the list &\n",
    "        # create vector from glove id number\n",
    "        seq.append([float(x) for x in content[int(i)].split()[1:]])\n",
    "        \n",
    "    return seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of the function:\n",
    "np.shape(glove_seq(find_glove(\"hello my friends, how are you\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'can', 'i', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'a', 'vpn'], ['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns']]\n",
      "(2, 20, 100)\n",
      "-6\n"
     ]
    }
   ],
   "source": [
    "# mess with some sentences / Questions from quora:\n",
    "\n",
    "q1 = \"How can I increase the speed of my internet connection while using a VPN?\"\n",
    "q2 = \"How can Internet speed be increased by hacking through DNS?\"\n",
    "qs = [q1,q2]\n",
    "\n",
    "# Keras imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "'''\n",
    "Preprocess the text and get a padded sequence of glove inputs \n",
    "for each question...\n",
    "'''\n",
    "# get each tokenized sentence and their glove idxs:\n",
    "questions = []\n",
    "glov_qs = []\n",
    "seq = []\n",
    "for i in qs:\n",
    "    tmp1,tmp2 = find_glove(i)\n",
    "    questions.append(tmp2)\n",
    "    # check that there is the word in the corpus \n",
    "    if True == True: # checking statement goes here\n",
    "        # make the sequences:\n",
    "        glov_qs.append(tmp1)\n",
    "        # convert our integer seq, to seq of glove vectors:\n",
    "        seq.append(glove_seq(tmp1))\n",
    "        \n",
    "    else:\n",
    "        # if it's not in corpus, then we need to \n",
    "        # model the closest word by characters / context \n",
    "        # do this at a later date\n",
    "        continue\n",
    "    \n",
    "print(questions)\n",
    "\n",
    "# Pad out the sequences...\n",
    "data = pad_sequences(seq, maxlen=20)\n",
    "print(np.shape(data))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
